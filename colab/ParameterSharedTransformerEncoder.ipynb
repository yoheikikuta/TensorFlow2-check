{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParameterSharedTransformerEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoheikikuta/TensorFlow2-check/blob/master/colab/ParameterSharedTransformerEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9W5XBXP0zdA",
        "colab_type": "text"
      },
      "source": [
        "# Parameter-sharing Transformer Encoder\n",
        "\n",
        "It can be run, but is not validated yet..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_QZkYrHPerZ",
        "colab_type": "code",
        "outputId": "d0ca5138-c9ff-4954-e904-226188740032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouWKXyP9Pjy2",
        "colab_type": "code",
        "outputId": "b8d631b4-94ac-4d99-83a6-97e0e1fda3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Veg8REyPlQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import six"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uWdA578SCG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A2jAhfVPsXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gelu(x):\n",
        "    \"\"\"Gaussian Error Linear Unit.\n",
        "    This is a smoother version of the RELU.\n",
        "    Original paper: https://arxiv.org/abs/1606.08415\n",
        "    Args:\n",
        "    x: float Tensor to perform activation.\n",
        "    Returns:\n",
        "    `x` with the GELU activation applied.\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "    return x * cdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvwwj7w3P6uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "    \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "    Args:\n",
        "    tensor: A tf.Tensor object to find the shape of.\n",
        "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "        specified and the `tensor` has a different rank, and exception will be\n",
        "        thrown.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "    Returns:\n",
        "    A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    be returned as python integers, and dynamic dimensions will be returned\n",
        "    as tf.Tensor scalars.\n",
        "    \"\"\"\n",
        "    # if name is None:\n",
        "    #     name = tensor.name\n",
        "\n",
        "    # if expected_rank is not None:\n",
        "    #     assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "    shape = tensor.shape.as_list()\n",
        "\n",
        "    non_static_indexes = []\n",
        "    for (index, dim) in enumerate(shape):\n",
        "        if dim is None:\n",
        "            non_static_indexes.append(index)\n",
        "\n",
        "    if not non_static_indexes:\n",
        "        return shape\n",
        "\n",
        "    dyn_shape = tf.shape(tensor)\n",
        "    for index in non_static_indexes:\n",
        "        shape[index] = dyn_shape[index]\n",
        "    return shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOW-ZcF7QE0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_to_matrix(input_tensor):\n",
        "    \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
        "    ndims = input_tensor.shape.ndims\n",
        "    if ndims < 2:\n",
        "        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                        (input_tensor.shape))\n",
        "    if ndims == 2:\n",
        "        return input_tensor\n",
        "\n",
        "    width = input_tensor.shape[-1]\n",
        "    output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "    return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "    \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
        "    if len(orig_shape_list) == 2:\n",
        "        return output_tensor\n",
        "\n",
        "    output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "    orig_dims = orig_shape_list[0:-1]\n",
        "    width = output_shape[-1]\n",
        "\n",
        "    return tf.reshape(output_tensor, orig_dims + [width])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AovnSLbfVPav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_initializer(initializer_range=0.02):\n",
        "    \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
        "    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YMzAO2Zhrhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout(input_tensor, dropout_prob):\n",
        "    \"\"\"Perform dropout.\n",
        "    Args:\n",
        "    input_tensor: float Tensor.\n",
        "    dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
        "        *keeping* a dimension as in `tf.nn.dropout`).\n",
        "    Returns:\n",
        "    A version of `input_tensor` with dropout applied.\n",
        "    \"\"\"\n",
        "    if dropout_prob is None or dropout_prob == 0.0:\n",
        "        return input_tensor\n",
        "\n",
        "    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYNNMCJru8aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer_norm(input_tensor, name=None):\n",
        "    \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
        "    return tf.keras.layers.LayerNormalization()(input_tensor)\n",
        "    # return tf.contrib.layers.layer_norm(\n",
        "    #     inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIIiHJySQrPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(layers.Layer):\n",
        "    \"\"\"Attention layer\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_attention_heads=1,\n",
        "                 size_per_head=512,\n",
        "                 query_act=None,\n",
        "                 key_act=None,\n",
        "                 value_act=None,\n",
        "                 initializer_range=0.02,\n",
        "                 attention_probs_dropout_prob=0.0,\n",
        "                 do_return_2d_tensor=False,\n",
        "                 name=\"attention\",\n",
        "                 **kwargs):\n",
        "        # Scalar dimensions referenced here:\n",
        "        #   B = batch size (number of sequences)\n",
        "        #   F = `from_tensor` sequence length\n",
        "        #   T = `to_tensor` sequence length\n",
        "        #   N = `num_attention_heads`\n",
        "        #   H = `size_per_head`\n",
        "        super(Attention, self).__init__(name=name, **kwargs)\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.size_per_head = size_per_head\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.do_return_2d_tensor = do_return_2d_tensor\n",
        "\n",
        "        # `query_layer` = [B*F, N*H]\n",
        "        self.query_layer = layers.Dense(\n",
        "            units=self.num_attention_heads * self.size_per_head,\n",
        "            activation=query_act,\n",
        "            name=\"query\",\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        # `key_layer` = [B*T, N*H]\n",
        "        self.key_layer = layers.Dense(\n",
        "            units=self.num_attention_heads * self.size_per_head,\n",
        "            activation=key_act,\n",
        "            name=\"key\",\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        # `value_layer` = [B*T, N*H]\n",
        "        self.value_layer = layers.Dense(\n",
        "            units=self.num_attention_heads * self.size_per_head,\n",
        "            activation=value_act,\n",
        "            name=\"value\",\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "\n",
        "    def call(self, from_tensor, to_tensor, attention_mask=None,\n",
        "             batch_size=None, from_seq_length=None, to_seq_length=None):\n",
        "        from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "        to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "        if len(from_shape) != len(to_shape):\n",
        "            raise ValueError(\n",
        "                \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "        if len(from_shape) == 3:\n",
        "            batch_size = from_shape[0]\n",
        "            from_seq_length = from_shape[1]\n",
        "            to_seq_length = to_shape[1]\n",
        "        elif len(from_shape) == 2:\n",
        "            if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "                raise ValueError(\n",
        "                    \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "                    \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "                    \"must all be specified.\")\n",
        "\n",
        "        from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "        to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "        # `query_layer` = [B, N, F, H]\n",
        "        query_layer = self.query_layer(from_tensor_2d)\n",
        "        query_layer = self.transpose_for_scores(query_layer, batch_size,\n",
        "                                            self.num_attention_heads, from_seq_length,\n",
        "                                            self.size_per_head)\n",
        "\n",
        "        # `key_layer` = [B, N, T, H]\n",
        "        key_layer = self.key_layer(to_tensor_2d)\n",
        "        key_layer = self.transpose_for_scores(key_layer, batch_size, self.num_attention_heads,\n",
        "                                        to_seq_length, self.size_per_head)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        # `attention_scores` = [B, N, F, T]\n",
        "        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
        "        attention_scores = tf.multiply(attention_scores,\n",
        "                                       1.0 / math.sqrt(float(self.size_per_head)))\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # `attention_mask` = [B, 1, F, T]\n",
        "            attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
        "\n",
        "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "            # masked positions, this operation will create a tensor which is 0.0 for\n",
        "            # positions we want to attend and -10000.0 for masked positions.\n",
        "            adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "            # Since we are adding it to the raw scores before the softmax, this is\n",
        "            # effectively the same as removing these entirely.\n",
        "            attention_scores += adder\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        # `attention_probs` = [B, N, F, T]\n",
        "        attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = dropout(attention_probs, self.attention_probs_dropout_prob)\n",
        "\n",
        "        # `value_layer` = [B, T, N, H]\n",
        "        value_layer = self.value_layer(to_tensor_2d)\n",
        "        # value_layer = layers.Reshape([batch_size, to_seq_length,\n",
        "        #                               self.num_attention_heads, self.size_per_head])(value_layer)\n",
        "        value_layer = layers.Reshape([to_seq_length,\n",
        "                                      self.num_attention_heads, self.size_per_head])(value_layer)\n",
        "\n",
        "        # `value_layer` = [B, N, T, H]\n",
        "        value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "        # `context_layer` = [B, N, F, H]\n",
        "        context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "        # `context_layer` = [B, F, N, H]\n",
        "        context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "        if self.do_return_2d_tensor:\n",
        "            # `context_layer` = [B*F, N*H]\n",
        "            context_layer = tf.reshape(\n",
        "                context_layer,\n",
        "                [batch_size * from_seq_length, self.num_attention_heads * self.size_per_head])\n",
        "        else:\n",
        "            # `context_layer` = [B, F, N*H]\n",
        "            context_layer = tf.reshape(\n",
        "                context_layer,\n",
        "                [batch_size, from_seq_length, self.num_attention_heads * self.size_per_head])\n",
        "\n",
        "        return context_layer\n",
        "\n",
        "    def transpose_for_scores(self, input_tensor, batch_size, num_attention_heads,\n",
        "                             seq_length, width):\n",
        "        # output_tensor = layers.Reshape([batch_size, seq_length,\n",
        "        #                                 num_attention_heads, width])(input_tensor)\n",
        "        output_tensor = layers.Reshape([seq_length, num_attention_heads, width])(input_tensor)\n",
        "        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "        return output_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDMPQcg6Zwl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = Attention()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv1nZ-x6ayLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_from = tf.constant([[[i for i in range(712)]]])\n",
        "dummy_to = tf.constant([[[i for i in range(712)]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "950uQqnJa4oI",
        "colab_type": "code",
        "outputId": "68785594-4622-4c9b-895f-970d8a926f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "test(dummy_from, dummy_to).shape"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 1, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5JOtyw3Pmei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ParameterSharedTransformerEncoder(tf.keras.Model):\n",
        "    \"\"\"Transformer Encoder model with parameter sharing\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 attention_mask=None,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 intermediate_act_fn=gelu,\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 initializer_range=0.02,\n",
        "                 do_return_all_layers=False,\n",
        "                 name=\"transformer_encoder\",\n",
        "                 **kwargs):\n",
        "        super(ParameterSharedTransformerEncoder, self).__init__(name=name, **kwargs)\n",
        "        self.attention_mask = attention_mask\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.intermediate_act_fn = intermediate_act_fn\n",
        "        self.num_attention_heads = int(self.hidden_size / 64)\n",
        "        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
        "        self.intermediate_size = int(4 * self.hidden_size)\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.initializer_range = initializer_range\n",
        "        self.do_return_all_layers = do_return_all_layers\n",
        "        self.attention_layer = Attention(\n",
        "            self.num_attention_heads,\n",
        "            self.attention_head_size,\n",
        "            do_return_2d_tensor=True)\n",
        "        self.attention_output_layer = layers.Dense(\n",
        "            self.hidden_size,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        self.intermediate_layer = layers.Dense(\n",
        "            self.intermediate_size,\n",
        "            activation=self.intermediate_act_fn,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        self.output_layer = layers.Dense(\n",
        "            self.hidden_size,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        input_shape = self.get_input_tensor_shape(input_tensor)\n",
        "        batch_size, seq_length, input_width = input_shape\n",
        "        prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "        all_layer_outputs = []\n",
        "        for layer_idx in range(self.num_hidden_layers):\n",
        "            layer_input = prev_output\n",
        "\n",
        "            ### Attention\n",
        "            attention_heads = []\n",
        "            attention_head = self.attention_layer(\n",
        "                layer_input,\n",
        "                layer_input,\n",
        "                self.attention_mask,\n",
        "                batch_size,\n",
        "                seq_length,\n",
        "                seq_length)\n",
        "            attention_heads.append(attention_head)\n",
        "            attention_output = None\n",
        "            if len(attention_heads) == 1:\n",
        "                attention_output = attention_heads[0]\n",
        "            else:\n",
        "                # In the case where we have other sequences, we just concatenate\n",
        "                # them to the self-attention head before the projection.\n",
        "                attention_output = tf.concat(attention_heads, axis=-1)\n",
        "            ### Attention output\n",
        "            attention_output = self.attention_output_layer(attention_output)\n",
        "            attention_output = dropout(attention_output, self.hidden_dropout_prob)\n",
        "            attention_output = layer_norm(attention_output + layer_input)\n",
        "            ### Intermediate\n",
        "            intermediate_output = self.intermediate_layer(attention_output)\n",
        "            ### Output\n",
        "            layer_output = self.output_layer(intermediate_output)\n",
        "            layer_output = dropout(layer_output, self.hidden_dropout_prob)\n",
        "            layer_output = layer_norm(layer_output + attention_output)\n",
        "            prev_output = layer_output\n",
        "\n",
        "            all_layer_outputs.append(layer_output)\n",
        "\n",
        "        if self.do_return_all_layers:\n",
        "            final_outputs = []\n",
        "            for layer_output in all_layer_outputs:\n",
        "                final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "                final_outputs.append(final_output)\n",
        "            return final_outputs\n",
        "        else:\n",
        "            final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "            return final_output\n",
        "\n",
        "\n",
        "    def get_input_tensor_shape(self, input_tensor):\n",
        "        input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "        # batch_size = input_shape[0]\n",
        "        # seq_length = input_shape[1]\n",
        "        input_width = input_shape[2]\n",
        "\n",
        "        # The Transformer performs sum residuals on all layers so the input needs\n",
        "        # to be the same as the hidden size.\n",
        "        if input_width != self.hidden_size:\n",
        "            raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
        "                                (input_width, self.hidden_size))\n",
        "        \n",
        "        # return batch_size, seq_length, input_width\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3F2aHEGPoj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = ParameterSharedTransformerEncoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCmTA-FRtW2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0d5d24f9-ab91-487c-a4b5-4e1f70dfa571"
      },
      "source": [
        "test.__dict__[\"num_attention_heads\"], test.__dict__[\"attention_head_size\"]"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-onkOqSqRs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0b005995-c0f2-4d10-a23f-871612ae5808"
      },
      "source": [
        "# dummy_input = tf.constant([[[i for i in range(768)], [i for i in range(768)]], [[i for i in range(768)], [i for i in range(768)]]])\n",
        "dummy_input = tf.constant([[[i for i in range(768)]], [[i for i in range(768)]]], dtype=tf.float32)\n",
        "dummy_input.shape"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXx4e1eUP3hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = test(tf.constant(dummy_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZr5HEMhQLb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f6f1b924-5b34-466f-e5fa-ec74d119dba2"
      },
      "source": [
        "result.shape"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnE22_pOSXJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-w7haG4SXM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzLb-FC_SXQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg9Wnhu40xmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR54lKf0xpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uslp0bFp0xv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Mm1QFASXg8",
        "colab_type": "text"
      },
      "source": [
        "# Trial and Errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4pPYAPPSZM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention_layer(from_tensor,\n",
        "                    to_tensor,\n",
        "                    attention_mask=None,\n",
        "                    num_attention_heads=1,\n",
        "                    size_per_head=512,\n",
        "                    query_act=None,\n",
        "                    key_act=None,\n",
        "                    value_act=None,\n",
        "                    attention_probs_dropout_prob=0.0,\n",
        "                    initializer_range=0.02,\n",
        "                    do_return_2d_tensor=False,\n",
        "                    batch_size=None,\n",
        "                    from_seq_length=None,\n",
        "                    to_seq_length=None):\n",
        "\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
        "                           seq_length, width):\n",
        "    output_tensor = tf.reshape(\n",
        "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\n",
        "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "\n",
        "  # Scalar dimensions referenced here:\n",
        "  #   B = batch size (number of sequences)\n",
        "  #   F = `from_tensor` sequence length\n",
        "  #   T = `to_tensor` sequence length\n",
        "  #   N = `num_attention_heads`\n",
        "  #   H = `size_per_head`\n",
        "\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  # `query_layer` = [B*F, N*H]\n",
        "  query_layer = tf.layers.dense(\n",
        "      from_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=query_act,\n",
        "      name=\"query\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `key_layer` = [B*T, N*H]\n",
        "  key_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=key_act,\n",
        "      name=\"key\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `value_layer` = [B*T, N*H]\n",
        "  value_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=value_act,\n",
        "      name=\"value\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `query_layer` = [B, N, F, H]\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size,\n",
        "                                     num_attention_heads, from_seq_length,\n",
        "                                     size_per_head)\n",
        "\n",
        "  # `key_layer` = [B, N, T, H]\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                   to_seq_length, size_per_head)\n",
        "\n",
        "  # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "  # attention scores.\n",
        "  # `attention_scores` = [B, N, F, T]\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
        "  attention_scores = tf.multiply(attention_scores,\n",
        "                                 1.0 / math.sqrt(float(size_per_head)))\n",
        "\n",
        "  if attention_mask is not None:\n",
        "    # `attention_mask` = [B, 1, F, T]\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
        "\n",
        "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "    # masked positions, this operation will create a tensor which is 0.0 for\n",
        "    # positions we want to attend and -10000.0 for masked positions.\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "    # Since we are adding it to the raw scores before the softmax, this is\n",
        "    # effectively the same as removing these entirely.\n",
        "    attention_scores += adder\n",
        "\n",
        "  # Normalize the attention scores to probabilities.\n",
        "  # `attention_probs` = [B, N, F, T]\n",
        "  attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "  # This is actually dropping out entire tokens to attend to, which might\n",
        "  # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  # `value_layer` = [B, T, N, H]\n",
        "  value_layer = tf.reshape(\n",
        "      value_layer,\n",
        "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
        "\n",
        "  # `value_layer` = [B, N, T, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "  # `context_layer` = [B, N, F, H]\n",
        "  context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "  # `context_layer` = [B, F, N, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    # `context_layer` = [B*F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
        "  else:\n",
        "    # `context_layer` = [B, F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
        "\n",
        "  return context_layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}